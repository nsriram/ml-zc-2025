{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download the model files",
   "id": "e144583692871b9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-10T16:56:32.233988Z",
     "start_time": "2025-12-10T16:56:27.251793Z"
    }
   },
   "source": "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 22:26:27--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\r\n",
      "Resolving github.com (github.com)... 20.207.73.82\r\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T17%3A40%3A44Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T16%3A40%3A43Z&ske=2025-12-10T17%3A40%3A44Z&sks=b&skv=2018-11-09&sig=KWnAh8N362gQdL2MGOIZur73jj3GNyXRQBjDnhh6Z7Q%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM4NzU4NywibmJmIjoxNzY1Mzg1Nzg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.RecBTx_DC25uK26aooyacjZBlYcilPCRHnuCBbDIpyg&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2025-12-10 22:26:27--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T17%3A40%3A44Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T16%3A40%3A43Z&ske=2025-12-10T17%3A40%3A44Z&sks=b&skv=2018-11-09&sig=KWnAh8N362gQdL2MGOIZur73jj3GNyXRQBjDnhh6Z7Q%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM4NzU4NywibmJmIjoxNzY1Mzg1Nzg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.RecBTx_DC25uK26aooyacjZBlYcilPCRHnuCBbDIpyg&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\r\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 80355328 (77M) [application/octet-stream]\r\n",
      "Saving to: ‘hair_classifier_v1.onnx.data’\r\n",
      "\r\n",
      "hair_classifier_v1. 100%[===================>]  76.63M  18.0MB/s    in 4.1s    \r\n",
      "\r\n",
      "2025-12-10 22:26:32 (18.7 MB/s) - ‘hair_classifier_v1.onnx.data’ saved [80355328/80355328]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:56:41.970527Z",
     "start_time": "2025-12-10T16:56:41.075151Z"
    }
   },
   "cell_type": "code",
   "source": "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx",
   "id": "c80a65d23a9eff79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 22:26:41--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\r\n",
      "Resolving github.com (github.com)... 20.207.73.82\r\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T17%3A39%3A04Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T16%3A38%3A39Z&ske=2025-12-10T17%3A39%3A04Z&sks=b&skv=2018-11-09&sig=vvUlSuuILXS9n%2FQY29GToYp0H3kAv8ERYqGtEL51fHk%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM4NjEwMSwibmJmIjoxNzY1Mzg1ODAxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.82byObPumcqZEArtnGmh8ClKCKK3ElTspQRPfs9if5Q&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2025-12-10 22:26:41--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T17%3A39%3A04Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T16%3A38%3A39Z&ske=2025-12-10T17%3A39%3A04Z&sks=b&skv=2018-11-09&sig=vvUlSuuILXS9n%2FQY29GToYp0H3kAv8ERYqGtEL51fHk%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM4NjEwMSwibmJmIjoxNzY1Mzg1ODAxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.82byObPumcqZEArtnGmh8ClKCKK3ElTspQRPfs9if5Q&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream\r\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 10337 (10K) [application/octet-stream]\r\n",
      "Saving to: ‘hair_classifier_v1.onnx’\r\n",
      "\r\n",
      "hair_classifier_v1. 100%[===================>]  10.09K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2025-12-10 22:26:41 (30.1 MB/s) - ‘hair_classifier_v1.onnx’ saved [10337/10337]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 1 : Output node name\n",
    "To be able to use this model, we need to know the name of the input and output nodes.\n",
    "What's the name of the output:\n",
    "\n",
    "- output\n",
    "- sigmoid\n",
    "- softmax\n",
    "- prediction"
   ],
   "id": "988f53d715476ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T08:23:59.588485Z",
     "start_time": "2025-12-11T08:23:58.829626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnxruntime as ort\n",
    "model_name = 'hair_classifier_v1.onnx'\n",
    "session = ort.InferenceSession(model_name, providers=['CPUExecutionProvider'])\n",
    "print('Output nodes:')\n",
    "for output_meta in session.get_outputs():\n",
    "    print(f'  Name: {output_meta.name}')\n",
    "    print(f'  shape: {output_meta.shape}')\n"
   ],
   "id": "61619f8d1a47c940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output nodes:\n",
      "  Name: output\n",
      "  shape: ['s77', 1]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Answer : 'output'",
   "id": "f7d70fb1b462be20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparing the image\n",
    "- Following code can download and resize images\n",
    "- Note : `pillow` already installed in the environment"
   ],
   "id": "478080175abfcd57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:06:31.691385Z",
     "start_time": "2025-12-10T17:06:31.630175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img"
   ],
   "id": "83d0fb1c12f55dcf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 2: Target size",
   "id": "34925fce2d56bbff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's download and resize this image",
   "id": "5fa15cf8d8ff4f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:07:55.791672Z",
     "start_time": "2025-12-10T17:07:53.719980Z"
    }
   },
   "cell_type": "code",
   "source": "! wget https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg",
   "id": "b21b9043723ec0e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 22:37:53--  https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\r\n",
      "Resolving habrastorage.org (habrastorage.org)... 95.47.173.34, 95.47.173.35\r\n",
      "Connecting to habrastorage.org (habrastorage.org)|95.47.173.34|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 398272 (389K) [image/jpeg]\r\n",
      "Saving to: ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg’\r\n",
      "\r\n",
      "yf_dokzqy3vcritme8g 100%[===================>] 388.94K   456KB/s    in 0.9s    \r\n",
      "\r\n",
      "2025-12-10 22:37:55 (456 KB/s) - ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg’ saved [398272/398272]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on the previous homework, what should be the target size for the image?\n",
    "- 64x64\n",
    "- 128x128\n",
    "- 200x200\n",
    "- 256x256\n"
   ],
   "id": "e60d0ba460bb38eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:11:27.585552Z",
     "start_time": "2025-12-10T17:11:27.582193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print input node names and shapes\n",
    "print('Input nodes:')\n",
    "for input_meta in session.get_inputs():\n",
    "  print(f'  Name: {input_meta.name}')\n",
    "  print(f'  Shape: {input_meta.shape}')\n"
   ],
   "id": "dea283a3b3f8d73a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input nodes:\n",
      "  Name: input\n",
      "  Shape: ['s77', 3, 200, 200]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Both from previous Homework and from the input shape, it can be seen that the target size is 200x200",
   "id": "d9d0303f0b4c26d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Answer : 200x200",
   "id": "bce6af19aba0b967"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "- Now we need to turn the image into numpy array and pre-process it.\n",
    "- Tip: Check the previous homework. What was the pre-processing we did there?"
   ],
   "id": "60ebe9bbb49ca64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "AAfter the pre-processing, what's the value in the first pixel, the R channel?\n",
    "- -10.73\n",
    "- -1.073\n",
    "- 1.073\n",
    "- 10.73\n"
   ],
   "id": "b0232e547a9dbb19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:15:48.550664Z",
     "start_time": "2025-12-10T17:15:48.505095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img = Image.open('yf_dokzqy3vcritme8ggnzqlvwa.jpeg')\n",
    "\n",
    "# Resize to 200x200\n",
    "img = img.resize((200, 200), Image.NEAREST)\n",
    "\n",
    "# Convert to numpy array and normalize to [0, 1] (same as ToTensor())\n",
    "x = np.array(img, dtype='float32')\n",
    "x = x / 255.0\n",
    "\n",
    "# Apply ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Normalize: (x - mean) / std\n",
    "x = (x - mean) / std\n",
    "\n",
    "# Get the first pixel, R channel (channel 0)\n",
    "first_pixel_r = x[0, 0, 0]\n",
    "\n",
    "print(f\"First pixel R channel value: {first_pixel_r:.3f}\")"
   ],
   "id": "6d731305f08dd833",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pixel R channel value: -1.073\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Answer : -1.073",
   "id": "8253a24c084fbe8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Now let's apply this model to this image. What's the output of the model?\n",
    "  - 0.09\n",
    "  - 0.49\n",
    "  - 0.69\n",
    "  - 0.89\n"
   ],
   "id": "e7a5dab888976aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:35:13.804247Z",
     "start_time": "2025-12-10T17:35:13.800675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras_image_helper import create_preprocessor\n",
    "\n",
    "def preprocess_pytorch_style(X):\n",
    "    # X: shape (1, 299, 299, 3), dtype=float32, values in [0, 255]\n",
    "    X = X / 255.0\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "\n",
    "    # Convert NHWC → NCHW\n",
    "    # from (batch, height, width, channels) → (batch, channels, height, width)\n",
    "    X = X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    # Normalize\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "preprocessor = create_preprocessor(\n",
    "    preprocess_pytorch_style,\n",
    "    target_size=(200, 200)\n",
    ")"
   ],
   "id": "355b5b42c1325931",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:35:16.156291Z",
     "start_time": "2025-12-10T17:35:14.684800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = 'https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg'\n",
    "X = preprocessor.from_url(url)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name"
   ],
   "id": "d26ce449b7e136c9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:35:18.661037Z",
     "start_time": "2025-12-10T17:35:18.636960Z"
    }
   },
   "cell_type": "code",
   "source": "result = session.run([output_name], {input_name: X})",
   "id": "71a2982a4bd02779",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:35:35.072397Z",
     "start_time": "2025-12-10T17:35:35.067967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "float_predictions = result[0][0].tolist()\n",
    "float_predictions"
   ],
   "id": "fc806f8a8e4f310a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0915662869811058]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Answer : 0.09",
   "id": "e0789a6687d6b3aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 5\n",
    "Download the base image `agrigorev/model-2025-hairstyle:v1`. You can do it with docker pull. So what's the size of this base image?\n",
    "\n",
    "- 88 Mb\n",
    "- 208 Mb\n",
    "- 608 Mb\n",
    "- 1208 Mb\n",
    "\n",
    "You can get this information when running `docker images` - it'll be in the \"SIZE\" column."
   ],
   "id": "c5ea929fe6109805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:42:08.399565Z",
     "start_time": "2025-12-10T17:41:57.421678Z"
    }
   },
   "cell_type": "code",
   "source": "! docker pull agrigorev/model-2025-hairstyle:v1",
   "id": "6a8c572ad6b00457",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: Pulling from agrigorev/model-2025-hairstyle\r\n",
      "\r\n",
      "\u001B[1B94d707b7: Pulling fs layer \r\n",
      "\u001B[1B9533db7f: Pulling fs layer \r\n",
      "\u001B[1B46295de2: Pulling fs layer \r\n",
      "\u001B[1Ba27bb275: Pulling fs layer \r\n",
      "\u001B[1Becca3b37: Pulling fs layer \r\n",
      "\u001B[1B39ee4ab8: Pulling fs layer \r\n",
      "\u001B[1Bd8a1e1c2: Pulling fs layer \r\n",
      "\u001B[8B94d707b7: Download complete MB/155.1MB\u001B[1A\u001B[2K\u001B[1A\u001B[2K\u001B[1A\u001B[2K\u001B[6A\u001B[2K\u001B[1A\u001B[2KDownloading     417B/417B\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2KDownloading  19.92MB/155.1MB\u001B[5A\u001B[2K\u001B[8A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[5A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2K\u001B[8A\u001B[2KDigest: sha256:9e43d5a5323f7f07688c0765d3c0137af66d0154af37833ed721d6b4de6df528\r\n",
      "Status: Downloaded newer image for agrigorev/model-2025-hairstyle:v1\r\n",
      "docker.io/agrigorev/model-2025-hairstyle:v1\r\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:44:13.399205Z",
     "start_time": "2025-12-10T17:44:13.152605Z"
    }
   },
   "cell_type": "code",
   "source": "! docker images",
   "id": "107667fa2d418ac8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                       TAG       IMAGE ID       CREATED        SIZE\r\n",
      "agrigorev/model-2025-hairstyle   v1        9e43d5a5323f   9 days ago     921MB\r\n",
      "agrigorev/zoomcamp-model         2025      14d79fde0bbf   7 weeks ago    181MB\r\n",
      "redis                            latest    f0957bcaa75f   2 months ago   222MB\r\n",
      "mysql                            latest    94254b456a6d   3 months ago   1.27GB\r\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Answer : 1208 MB\n",
    "\n",
    "Note :\n",
    "- On Macbook M3, the size is 921MB. Hence, choosing the nearest higher value 1208MB.\n",
    "- Though the compressed size on docker hub is only 256.51 MB, the downloaded and uncompressed size is larger."
   ],
   "id": "c37cfefb28cec109"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 6",
   "id": "d9d6a1847143db35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's extend this docker image, install all the required libraries and add the code for lambda.\n",
    "\n",
    "You don't need to include the model in the image. It's already included. The name of the file with the model is hair_classifier_empty.onnx and it's in the current workdir in the image (see the Dockerfile above for the reference). The provided model requires the same preprocessing for images regarding target size and rescaling the value range than used in homework 8.\n",
    "\n",
    "Now run the container locally.\n",
    "\n",
    "Score this image: https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
    "\n",
    "What's the output from the model?\n",
    "- -1.0\n",
    "- -0.10\n",
    "- 0.10\n",
    "- 1.0"
   ],
   "id": "8185d4beba87ae5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:05:37.667604Z",
     "start_time": "2025-12-10T18:05:37.525719Z"
    }
   },
   "cell_type": "code",
   "source": "! cat Dockerfile",
   "id": "7af31f987a66ed7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM agrigorev/model-2025-hairstyle:v1\r\n",
      "\r\n",
      "RUN pip install onnxruntime keras-image-helper==0.0.2\r\n",
      "\r\n",
      "ENV MODEL_NAME=hair_classifier_empty.onnx\r\n",
      "\r\n",
      "COPY lambda_function.py ./\r\n",
      "\r\n",
      "CMD [\"lambda_function.lambda_handler\"]\r\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:05:40.692616Z",
     "start_time": "2025-12-10T18:05:40.565260Z"
    }
   },
   "cell_type": "code",
   "source": "! cat lambda_function.py",
   "id": "30a7ab052dbd6076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import onnxruntime as ort\r\n",
      "from keras_image_helper import create_preprocessor\r\n",
      "\r\n",
      "model_name = 'hair_classifier_empty.onnx'\r\n",
      "\r\n",
      "def preprocess_pytorch_style(X):\r\n",
      "    X = X / 255.0\r\n",
      "\r\n",
      "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\r\n",
      "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\r\n",
      "\r\n",
      "    # Convert NHWC → NCHW\r\n",
      "    # from (batch, height, width, channels) → (batch, channels, height, width)\r\n",
      "    X = X.transpose(0, 3, 1, 2)\r\n",
      "\r\n",
      "    # Normalize\r\n",
      "    X = (X - mean) / std\r\n",
      "\r\n",
      "    return X.astype(np.float32)\r\n",
      "\r\n",
      "preprocessor = create_preprocessor(\r\n",
      "    preprocess_pytorch_style,\r\n",
      "    target_size=(200, 200)\r\n",
      ")\r\n",
      "\r\n",
      "session = ort.InferenceSession(\r\n",
      "    model_name, providers=[\"CPUExecutionProvider\"]\r\n",
      ")\r\n",
      "input_name = session.get_inputs()[0].name\r\n",
      "output_name = session.get_outputs()[0].name\r\n",
      "\r\n",
      "def predict(url):\r\n",
      "    X = preprocessor.from_url(url)\r\n",
      "    result = session.run([output_name], {input_name: X})\r\n",
      "    float_prediction = result[0][0][0]\r\n",
      "    return float_prediction\r\n",
      "\r\n",
      "def lambda_handler(event, context):\r\n",
      "    url = event[\"url\"]\r\n",
      "    result = predict(url)\r\n",
      "    return result\r\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build and run the docker container\n",
    "Build :\n",
    "`docker build --platform linux/amd64 -t hair-classifier .`\n",
    "\n",
    "Run :\n",
    "`docker run --platform linux/amd64 -it --rm -p 8080:8080 hair-classifier`"
   ],
   "id": "2e71c211857eb56f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:06:05.545375Z",
     "start_time": "2025-12-10T18:06:05.316647Z"
    }
   },
   "cell_type": "code",
   "source": "! docker images",
   "id": "2ea2a402a6749be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                       TAG       IMAGE ID       CREATED         SIZE\r\n",
      "hair-classifier                  latest    afe72ea573cc   9 seconds ago   387MB\r\n",
      "agrigorev/model-2025-hairstyle   v1        9e43d5a5323f   9 days ago      921MB\r\n",
      "agrigorev/zoomcamp-model         2025      14d79fde0bbf   7 weeks ago     181MB\r\n",
      "redis                            latest    f0957bcaa75f   2 months ago    222MB\r\n",
      "mysql                            latest    94254b456a6d   3 months ago    1.27GB\r\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:06:17.588531Z",
     "start_time": "2025-12-10T18:06:17.414472Z"
    }
   },
   "cell_type": "code",
   "source": "! docker ps",
   "id": "543d71fdc290f830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE             COMMAND                  CREATED         STATUS         PORTS                                         NAMES\r\n",
      "25414564f208   hair-classifier   \"/lambda-entrypoint.…\"   5 seconds ago   Up 3 seconds   0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp   lucid_mcnulty\r\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We can see that the container is created and is running",
   "id": "fa0eab8b33b4beb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:11:51.532815Z",
     "start_time": "2025-12-10T18:11:48.246849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
    "\n",
    "request = {\n",
    "    \"url\": \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "}\n",
    "\n",
    "result = requests.post(url, json=request).json()\n",
    "print(result)"
   ],
   "id": "a631e6c5aa313136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1022084578871727\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Answer : -0.10",
   "id": "4245168b165d685d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6cbfb7883db8de7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

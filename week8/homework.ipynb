{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 8 Homework - Deep Learning",
   "id": "8c64eb8cb472a99f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data download",
   "id": "9af6dd317312fc4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T19:21:13.319214Z",
     "start_time": "2025-12-02T19:21:00.461206Z"
    }
   },
   "source": [
    "# download the data\n",
    "!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-03 00:51:00--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\r\n",
      "Resolving github.com (github.com)... 20.207.73.82\r\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T20%3A16%3A16Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T19%3A15%3A54Z&ske=2025-12-02T20%3A16%3A16Z&sks=b&skv=2018-11-09&sig=zhWpKupN4am%2BDPQqkGXIAQ3M7BM15QianGuzQ7ZevtU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDcwNTA2MCwibmJmIjoxNzY0NzAzMjYwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.ilfRVP4JcY_3BMkLkRpC_zMlLstSMAa_8-FdN1ol_AM&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2025-12-03 00:51:01--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T20%3A16%3A16Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T19%3A15%3A54Z&ske=2025-12-02T20%3A16%3A16Z&sks=b&skv=2018-11-09&sig=zhWpKupN4am%2BDPQqkGXIAQ3M7BM15QianGuzQ7ZevtU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDcwNTA2MCwibmJmIjoxNzY0NzAzMjYwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.ilfRVP4JcY_3BMkLkRpC_zMlLstSMAa_8-FdN1ol_AM&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\r\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 102516572 (98M) [application/octet-stream]\r\n",
      "Saving to: ‘data.zip’\r\n",
      "\r\n",
      "data.zip            100%[===================>]  97.77M  8.45MB/s    in 11s     \r\n",
      "\r\n",
      "2025-12-03 00:51:13 (8.68 MB/s) - ‘data.zip’ saved [102516572/102516572]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:42:09.551481Z",
     "start_time": "2025-12-02T20:42:08.627569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# unzip the data\n",
    "!unzip -qq data.zip"
   ],
   "id": "41a126fc5b5a5b7b",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:49:57.961396Z",
     "start_time": "2025-12-02T19:49:57.959160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "8e6fa08f751cf521",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:31:05.882437Z",
     "start_time": "2025-12-02T19:31:05.878239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the random seed for reproducibility to be 42\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "id": "ec7735fee0b24c84",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up the CNN Model\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
    "* Next, create a convolutional layer (`nn.Conv2d`):\n",
    "    * Use 32 filters (output channels)\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter), padding = 0, stride = 1\n",
    "    * Use `'relu'` as activation\n",
    "* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using `flatten` or `view`\n",
    "* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use `torch.optim.SGD` with the following parameters:\n",
    "\n",
    "* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`\n"
   ],
   "id": "418db1beaa55af2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:39:15.510633Z",
     "start_time": "2025-12-02T19:39:15.507943Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Define the CNN Model\n",
    "class HairTypeCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(HairTypeCNN, self).__init__()\n",
    "\n",
    "      # Convolutional layer: 32 filters, kernel (3,3), padding=0, stride=1\n",
    "      self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,\n",
    "padding=0, stride=1)\n",
    "      self.relu1 = nn.ReLU()\n",
    "\n",
    "      # Max pooling layer: pooling size (2,2)\n",
    "      self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "      # After conv: (3, 200, 200) -> (32, 198, 198)\n",
    "      # After pool: (32, 198, 198) -> (32, 99, 99)\n",
    "      # Flattened size: 32 * 99 * 99 = 313,632\n",
    "\n",
    "      # Fully connected layers\n",
    "      self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
    "      self.relu2 = nn.ReLU()\n",
    "\n",
    "      # Output layer: 1 neuron for binary classification\n",
    "      self.fc2 = nn.Linear(64, 1)\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "      # Convolutional layer with ReLU\n",
    "      x = self.conv1(x)\n",
    "      x = self.relu1(x)\n",
    "\n",
    "      # Max pooling\n",
    "      x = self.pool(x)\n",
    "\n",
    "      # Flatten\n",
    "      x = x.view(x.size(0), -1)\n",
    "\n",
    "      # First fully connected layer with ReLU\n",
    "      x = self.fc1(x)\n",
    "      x = self.relu2(x)\n",
    "\n",
    "      # Output layer (returns logits, not probabilities)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      return x\n"
   ],
   "id": "83907af6c8b442c7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:39:22.537853Z",
     "start_time": "2025-12-02T19:39:22.474393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the model\n",
    "model = HairTypeCNN()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ],
   "id": "a25ace2c1a36b223",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HairTypeCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:40:38.490782Z",
     "start_time": "2025-12-02T19:40:38.488470Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ],
   "id": "c696b85244100e81",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 1\n",
    "Which loss function you will use?\n",
    "- nn.MSELoss()\n",
    "- nn.BCEWithLogitsLoss()\n",
    "- nn.CrossEntropyLoss()\n",
    "- nn.CosineEmbeddingLoss()\n",
    "(Multiple answered can be correct, so pick any)"
   ],
   "id": "200152469263d433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option B\n",
    "> `nn.BCEWithLogitsLoss()`\n",
    "\n",
    "- This is a binary classification problem classifying hair types into 2 categories\n",
    "- BCE is designed for binary classification tasks\n",
    "- WithLogits expects raw logits as input and applies sigmoid internally. Also the training code shared uses raw logits"
   ],
   "id": "539dcaf6e7c60b30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 2\n",
    "- What's the total number of parameters of the model? You can use torchsummary or count manually."
   ],
   "id": "70cb44c73a718f72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:45:55.109706Z",
     "start_time": "2025-12-02T19:45:55.084860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Or using torchsummary\n",
    "summary(model, input_size=(3, 200, 200))"
   ],
   "id": "cc008e6653e4b0d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 20,073,473\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "              ReLU-2         [-1, 32, 198, 198]               0\n",
      "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
      "            Linear-4                   [-1, 64]      20,072,512\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "            Linear-6                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 21.54\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 98.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option D\n",
    "> Total parameters: 20,073,473"
   ],
   "id": "77f2d6aac2955874"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generators and Training",
   "id": "582cab3b58dbe70f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:57:44.732023Z",
     "start_time": "2025-12-02T19:57:44.727922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "input_size = 200\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ],
   "id": "eb9807ce7adcc641",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:58:02.986637Z",
     "start_time": "2025-12-02T19:58:02.974867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the datasets\n",
    "train_dataset = datasets.ImageFolder(\n",
    "  root='data/train',\n",
    "  transform=train_transforms\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(\n",
    "  root='data/test',\n",
    "  transform=test_transforms\n",
    ")\n"
   ],
   "id": "d925dec66281f5d3",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:58:50.923328Z",
     "start_time": "2025-12-02T19:58:50.921303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# declare batch size as 20\n",
    "batch_size = 20\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset,batch_size=batch_size,shuffle=False)"
   ],
   "id": "90243296d27412d4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:59:03.317360Z",
     "start_time": "2025-12-02T19:59:03.314687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}"
   ],
   "id": "d524014f2bfdee45",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:01:40.207164Z",
     "start_time": "2025-12-02T20:01:40.205406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the loss function\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ],
   "id": "bf39c36a488b16fe",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:02:58.917915Z",
     "start_time": "2025-12-02T20:01:42.213425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ],
   "id": "b4a38be9831c8c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6370, Acc: 0.6250, Val Loss: 0.6774, Val Acc: 0.6269\n",
      "Epoch 2/10, Loss: 0.5508, Acc: 0.7288, Val Loss: 0.7913, Val Acc: 0.5721\n",
      "Epoch 3/10, Loss: 0.5044, Acc: 0.7538, Val Loss: 0.6268, Val Acc: 0.6468\n",
      "Epoch 4/10, Loss: 0.4535, Acc: 0.7788, Val Loss: 0.6964, Val Acc: 0.6667\n",
      "Epoch 5/10, Loss: 0.4061, Acc: 0.8250, Val Loss: 0.6546, Val Acc: 0.6567\n",
      "Epoch 6/10, Loss: 0.3270, Acc: 0.8550, Val Loss: 0.7766, Val Acc: 0.6766\n",
      "Epoch 7/10, Loss: 0.2732, Acc: 0.8862, Val Loss: 0.6556, Val Acc: 0.7313\n",
      "Epoch 8/10, Loss: 0.2504, Acc: 0.8938, Val Loss: 0.7788, Val Acc: 0.6915\n",
      "Epoch 9/10, Loss: 0.1882, Acc: 0.9150, Val Loss: 0.7553, Val Acc: 0.7065\n",
      "Epoch 10/10, Loss: 0.1658, Acc: 0.9287, Val Loss: 0.8221, Val Acc: 0.7164\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "- What is the median of training accuracy for all the epochs for this model?"
   ],
   "id": "49ca0dca364cd5ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:06:35.144511Z",
     "start_time": "2025-12-02T20:06:35.141148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_accuracies = history['acc']\n",
    "median_train_acc = np.median(train_accuracies)\n",
    "\n",
    "print(f\"Training accuracies for all epochs: {train_accuracies}\")\n",
    "print(f\"Median training accuracy: {median_train_acc:.4f}\")"
   ],
   "id": "c770c7bf3dd8aa33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracies for all epochs: [0.625, 0.72875, 0.75375, 0.77875, 0.825, 0.855, 0.88625, 0.89375, 0.915, 0.92875]\n",
      "Median training accuracy: 0.8400\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option D\n",
    "> Median training accuracy: 0.84"
   ],
   "id": "7248ffdc7a995cf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "- What is the standard deviation of training loss for all the epochs for this model?"
   ],
   "id": "ca9195cb630fdcf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:07:29.144791Z",
     "start_time": "2025-12-02T20:07:29.142759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses = history['loss']\n",
    "std_train_loss = np.std(train_losses)\n",
    "\n",
    "print(f\"Training losses for all epochs: {train_losses}\")\n",
    "print(f\"Standard deviation of training loss: {std_train_loss:.4f}\")"
   ],
   "id": "84112502aaad46b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses for all epochs: [0.6370332680642605, 0.5507597908377647, 0.5044267348945141, 0.4534881852567196, 0.40610496401786805, 0.32695389464497565, 0.27321023158729074, 0.25039489604532716, 0.18823296772316098, 0.16575322356075048]\n",
      "Standard deviation of training loss: 0.1518\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option C\n",
    "> Standard deviation of training loss: 0.171"
   ],
   "id": "75e93bde814ce439"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 5\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "> Note: make sure you don't re-create the model. we want to continue training the model we already started training.\n",
    "\n",
    "- What is the mean of test loss for all the epochs for the model trained with augmentations?"
   ],
   "id": "1e9172a0fa422a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:27:58.545035Z",
     "start_time": "2025-12-02T20:27:58.537768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define training transforms WITH augmentations\n",
    "train_transforms_aug = transforms.Compose([\n",
    "  transforms.RandomRotation(10),\n",
    "  transforms.RandomResizedCrop(200, scale=(0.9, 1.0)),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=mean,std=std)\n",
    "])\n",
    "\n",
    "# Create new training dataset with augmentations\n",
    "train_dataset_aug = datasets.ImageFolder(\n",
    "  root='data/train',\n",
    "  transform=train_transforms_aug\n",
    ")\n",
    "\n",
    "# Create new training data loader with augmentations\n",
    "train_loader_aug = DataLoader(\n",
    "  train_dataset_aug,\n",
    "  batch_size=20,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with augmentations: {len(train_dataset_aug)}\")"
   ],
   "id": "25ef481bbad28b9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with augmentations: 800\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:28:04.098268Z",
     "start_time": "2025-12-02T20:28:04.095785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs_aug = 10\n",
    "history_aug = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}"
   ],
   "id": "b2de54399a710c99",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:29:27.833315Z",
     "start_time": "2025-12-02T20:28:06.905074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs_aug):\n",
    "  model.train()\n",
    "  running_loss = 0.0\n",
    "  correct_train = 0\n",
    "  total_train = 0\n",
    "  for images, labels in train_loader_aug:  # Using augmented data loader\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      labels = labels.float().unsqueeze(1)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item() * images.size(0)\n",
    "      predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "      total_train += labels.size(0)\n",
    "      correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "  epoch_loss = running_loss / len(train_dataset_aug)\n",
    "  epoch_acc = correct_train / total_train\n",
    "  history_aug['loss'].append(epoch_loss)\n",
    "  history_aug['acc'].append(epoch_acc)\n",
    "\n",
    "  model.eval()\n",
    "  val_running_loss = 0.0\n",
    "  correct_val = 0\n",
    "  total_val = 0\n",
    "  with torch.no_grad():\n",
    "      for images, labels in validation_loader:\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          labels = labels.float().unsqueeze(1)\n",
    "\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          val_running_loss += loss.item() * images.size(0)\n",
    "          predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "          total_val += labels.size(0)\n",
    "          correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "  val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "  val_epoch_acc = correct_val / total_val\n",
    "  history_aug['val_loss'].append(val_epoch_loss)\n",
    "  history_aug['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs_aug}, \"\n",
    "        f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "        f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ],
   "id": "4b853b14efad19d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3860, Acc: 0.8350, Val Loss: 0.5634, Val Acc: 0.7015\n",
      "Epoch 2/10, Loss: 0.3751, Acc: 0.8237, Val Loss: 0.5407, Val Acc: 0.7114\n",
      "Epoch 3/10, Loss: 0.3987, Acc: 0.8125, Val Loss: 0.5936, Val Acc: 0.7114\n",
      "Epoch 4/10, Loss: 0.3445, Acc: 0.8375, Val Loss: 0.5273, Val Acc: 0.7313\n",
      "Epoch 5/10, Loss: 0.3404, Acc: 0.8538, Val Loss: 0.4737, Val Acc: 0.7363\n",
      "Epoch 6/10, Loss: 0.3234, Acc: 0.8650, Val Loss: 0.5328, Val Acc: 0.7413\n",
      "Epoch 7/10, Loss: 0.3353, Acc: 0.8462, Val Loss: 0.5298, Val Acc: 0.7562\n",
      "Epoch 8/10, Loss: 0.2957, Acc: 0.8688, Val Loss: 0.4971, Val Acc: 0.7413\n",
      "Epoch 9/10, Loss: 0.2943, Acc: 0.8625, Val Loss: 0.5023, Val Acc: 0.7512\n",
      "Epoch 10/10, Loss: 0.2800, Acc: 0.8862, Val Loss: 0.4815, Val Acc: 0.7761\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:32:59.262847Z",
     "start_time": "2025-12-02T20:32:59.260131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_val_losses = history['val_loss'] + history_aug['val_loss']\n",
    "mean_all_val_loss = np.mean(all_val_losses)\n",
    "\n",
    "print(\"\\n--- Question 5 (All 20 epochs) ---\")\n",
    "print(f\"Validation losses from initial 10 epochs: {history['val_loss']}\")\n",
    "print(f\"Validation losses from augmented 10 epochs: {history_aug['val_loss']}\")\n",
    "print(f\"All validation losses (20 epochs): {all_val_losses}\")\n",
    "print(f\"Mean of test/validation loss (all 20 epochs): {mean_all_val_loss:.4f}\")\n",
    "\n"
   ],
   "id": "c3c6d42410c7fc0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 5 (All 20 epochs) ---\n",
      "Validation losses from initial 10 epochs: [0.6774433258902374, 0.7912714639261588, 0.6268203786356532, 0.6964370359235735, 0.6546140087777702, 0.7765639792034282, 0.6556346054693952, 0.7788022706164649, 0.7553474221063491, 0.8220905878057527]\n",
      "Validation losses from augmented 10 epochs: [0.5633561821719307, 0.5406737262336769, 0.5935937659953957, 0.5272957721159826, 0.47369566544964536, 0.5327601429834887, 0.5297813961161902, 0.49710180895838574, 0.5023328564060268, 0.48154821087471883]\n",
      "All validation losses (20 epochs): [0.6774433258902374, 0.7912714639261588, 0.6268203786356532, 0.6964370359235735, 0.6546140087777702, 0.7765639792034282, 0.6556346054693952, 0.7788022706164649, 0.7553474221063491, 0.8220905878057527, 0.5633561821719307, 0.5406737262336769, 0.5935937659953957, 0.5272957721159826, 0.47369566544964536, 0.5327601429834887, 0.5297813961161902, 0.49710180895838574, 0.5023328564060268, 0.48154821087471883]\n",
      "Mean of test/validation loss (all 20 epochs): 0.6239\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option C\n",
    "> mean of test loss for all the epochs for the model trained with augmentations : 0.88"
   ],
   "id": "1832fcbdae1ec649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 6\n",
    "- What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?"
   ],
   "id": "8252649b3064eca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:34:53.432078Z",
     "start_time": "2025-12-02T20:34:53.430147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "last_5_val_acc = history_aug['val_acc'][5:10]\n",
    "avg_last_5_val_acc = np.mean(last_5_val_acc)\n",
    "print(f\"Average test accuracy for last 5 epochs: {avg_last_5_val_acc:.4f}\")"
   ],
   "id": "8645d499bcabb6b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy for last 5 epochs: 0.7532\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer : Option C\n",
    "> Average test accuracy for last 5 epochs: 0.68"
   ],
   "id": "61c0b22a9418356f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "764afa8a8550fa85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
